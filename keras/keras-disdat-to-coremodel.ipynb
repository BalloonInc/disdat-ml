{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Keras version 2.0.6 detected. Last version known to be fully compatible of Keras is 2.0.4 .\n",
      "WARNING:root:TensorFlow version 1.3.0 detected. Last version known to be fully compatible is 1.1.1 .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import operator\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.callbacks\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = 'disdat-104-v5-softmax-lr0.001-decay0'\n",
    "label_file='labels.txt'\n",
    "data_folder='../clean-data'\n",
    "output='disdatkerasv7'\n",
    "num_classes=104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape=(7,7,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 115042 images belonging to 104 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "data_generator = datagen.flow_from_directory(\n",
    "    data_folder,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=11,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "class_dictionary = data_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_predictions = sorted(class_dictionary.items(), key=operator.itemgetter(1))\n",
    "labels=[p[0] for p in sorted_predictions]\n",
    "\n",
    "with open(label_file, 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write(label+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(weights+'.config') as config_file:    \n",
    "    config=json.load(config_file)\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(num_classes, activation='softmax'))\n",
    "decay=1/100 * (config['lr'])/(100000/config['batch_size'])\n",
    "top_model.compile(optimizer=SGD(lr=config['lr'], decay=config['decay']),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "top_model.load_weights(weights+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs= base_model.input, outputs= top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : input_1, <keras.engine.topology.InputLayer object at 0x7f93148a59d0>\n",
      "1 : block1_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92eca44750>\n",
      "2 : block1_conv1__activation__, <keras.layers.core.Activation object at 0x7f92dd4fafd0>\n",
      "3 : block1_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd896290>\n",
      "4 : block1_conv2__activation__, <keras.layers.core.Activation object at 0x7f92db9ee1d0>\n",
      "5 : block1_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dd896d90>\n",
      "6 : block2_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92dd83df50>\n",
      "7 : block2_conv1__activation__, <keras.layers.core.Activation object at 0x7f92d9f04e90>\n",
      "8 : block2_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd852810>\n",
      "9 : block2_conv2__activation__, <keras.layers.core.Activation object at 0x7f92dd519190>\n",
      "10 : block2_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dd86ed50>\n",
      "11 : block3_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92dd815d10>\n",
      "12 : block3_conv1__activation__, <keras.layers.core.Activation object at 0x7f92dd5191d0>\n",
      "13 : block3_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd824b90>\n",
      "14 : block3_conv2__activation__, <keras.layers.core.Activation object at 0x7f92dd519310>\n",
      "15 : block3_conv3, <keras.layers.convolutional.Conv2D object at 0x7f92dd7ba990>\n",
      "16 : block3_conv3__activation__, <keras.layers.core.Activation object at 0x7f92dd519390>\n",
      "17 : block3_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dd7ceb10>\n",
      "18 : block4_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92dd79ec90>\n",
      "19 : block4_conv1__activation__, <keras.layers.core.Activation object at 0x7f92dd519410>\n",
      "20 : block4_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd79ef50>\n",
      "21 : block4_conv2__activation__, <keras.layers.core.Activation object at 0x7f92dd519490>\n",
      "22 : block4_conv3, <keras.layers.convolutional.Conv2D object at 0x7f92dd735fd0>\n",
      "23 : block4_conv3__activation__, <keras.layers.core.Activation object at 0x7f92dd519510>\n",
      "24 : block4_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dd74aed0>\n",
      "25 : block5_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92dd6f1fd0>\n",
      "26 : block5_conv1__activation__, <keras.layers.core.Activation object at 0x7f92dd519590>\n",
      "27 : block5_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd701750>\n",
      "28 : block5_conv2__activation__, <keras.layers.core.Activation object at 0x7f92dd519610>\n",
      "29 : block5_conv3, <keras.layers.convolutional.Conv2D object at 0x7f92dd716c90>\n",
      "30 : block5_conv3__activation__, <keras.layers.core.Activation object at 0x7f92dd519690>\n",
      "31 : block5_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dd729dd0>\n",
      "32 : sequential_1_flatten_1, <keras.layers.core.Flatten object at 0x7f92db9fcc50>\n",
      "33 : sequential_1_dense_1, <keras.layers.core.Dense object at 0x7f92db9fb5d0>\n",
      "34 : sequential_1_dense_1__activation__, <keras.layers.core.Activation object at 0x7f92dd5196d0>\n",
      "35 : sequential_1_dense_2, <keras.layers.core.Dense object at 0x7f92dd68bf10>\n",
      "36 : sequential_1_dense_2__activation__, <keras.layers.core.Activation object at 0x7f92dd519710>\n"
     ]
    }
   ],
   "source": [
    "coreml_model  = coremltools.converters.keras.convert(model,\n",
    "                                                    image_input_names='image',\n",
    "                                                    class_labels=label_file,\n",
    "                                                    input_names=['image'],  \n",
    "#                                                     red_bias=-123.682/255,\n",
    "#                                                     green_bias=-116.78/255,\n",
    "#                                                     blue_bias=-103.94/255,\n",
    "                                                    image_scale=1/255.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coreml_model.save(output+'top.mlmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "#model.load_weights(weights+'-refined.h5')\n",
    "model.load_weights('disdat-104-v5-softmax-lr0.001-decay0-refined-ep12-valacc0.75.h5')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=2e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : input_3, <keras.engine.topology.InputLayer object at 0x7f92dd165490>\n",
      "1 : block1_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92dd165550>\n",
      "2 : block1_conv1__activation__, <keras.layers.core.Activation object at 0x7f92dd1650d0>\n",
      "3 : block1_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd1655d0>\n",
      "4 : block1_conv2__activation__, <keras.layers.core.Activation object at 0x7f92db9aed50>\n",
      "5 : block1_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dd1658d0>\n",
      "6 : block2_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92dd15ae50>\n",
      "7 : block2_conv1__activation__, <keras.layers.core.Activation object at 0x7f92db94a0d0>\n",
      "8 : block2_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd15af90>\n",
      "9 : block2_conv2__activation__, <keras.layers.core.Activation object at 0x7f92db94a110>\n",
      "10 : block2_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dd0b0d10>\n",
      "11 : block3_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92dd0e7b10>\n",
      "12 : block3_conv1__activation__, <keras.layers.core.Activation object at 0x7f92db94a190>\n",
      "13 : block3_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd10ef10>\n",
      "14 : block3_conv2__activation__, <keras.layers.core.Activation object at 0x7f92db94a210>\n",
      "15 : block3_conv3, <keras.layers.convolutional.Conv2D object at 0x7f92dd104e90>\n",
      "16 : block3_conv3__activation__, <keras.layers.core.Activation object at 0x7f92db94a290>\n",
      "17 : block3_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dd076210>\n",
      "18 : block4_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92dd09db90>\n",
      "19 : block4_conv1__activation__, <keras.layers.core.Activation object at 0x7f92db94a310>\n",
      "20 : block4_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd0334d0>\n",
      "21 : block4_conv2__activation__, <keras.layers.core.Activation object at 0x7f92db94a390>\n",
      "22 : block4_conv3, <keras.layers.convolutional.Conv2D object at 0x7f92dd045850>\n",
      "23 : block4_conv3__activation__, <keras.layers.core.Activation object at 0x7f92db94a410>\n",
      "24 : block4_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dd05a4d0>\n",
      "25 : block5_conv1, <keras.layers.convolutional.Conv2D object at 0x7f92dd013ed0>\n",
      "26 : block5_conv1__activation__, <keras.layers.core.Activation object at 0x7f92db94a490>\n",
      "27 : block5_conv2, <keras.layers.convolutional.Conv2D object at 0x7f92dd028c50>\n",
      "28 : block5_conv2__activation__, <keras.layers.core.Activation object at 0x7f92db94a510>\n",
      "29 : block5_conv3, <keras.layers.convolutional.Conv2D object at 0x7f92dcfb9ed0>\n",
      "30 : block5_conv3__activation__, <keras.layers.core.Activation object at 0x7f92db94a590>\n",
      "31 : block5_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f92dcfcec50>\n",
      "32 : sequential_3_flatten_3, <keras.layers.core.Flatten object at 0x7f92dcf86d90>\n",
      "33 : sequential_3_dense_5, <keras.layers.core.Dense object at 0x7f92dcf860d0>\n",
      "34 : sequential_3_dense_5__activation__, <keras.layers.core.Activation object at 0x7f92db94a5d0>\n",
      "35 : sequential_3_dense_6, <keras.layers.core.Dense object at 0x7f92dcf9ce50>\n",
      "36 : sequential_3_dense_6__activation__, <keras.layers.core.Activation object at 0x7f92db94a610>\n"
     ]
    }
   ],
   "source": [
    "coreml_model  = coremltools.converters.keras.convert(model,\n",
    "                                                    image_input_names='image',\n",
    "                                                    class_labels=label_file,\n",
    "                                                    input_names=['image'],  \n",
    "#                                                     red_bias=-123.68/255,\n",
    "#                                                     green_bias=-116.78/255,\n",
    "#                                                     blue_bias=-103.94/255,\n",
    "                                                    image_scale=1/255.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coreml_model.save(output+'.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting calculator.jpg\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "[('calculator', 0.98614025), ('keyboard', 0.005407881), ('hairbrush', 0.0020927957), ('clock', 0.0010452823)]\n",
      "[('calculator', 0.99983561), ('keyboard', 0.00013622294), ('puzzle', 1.6809556e-05), ('computer mouse', 2.7440112e-06)]\n",
      "Predicting dog.jpg\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "[('cat', 0.39209151), ('dog', 0.30477524), ('sofa', 0.049107771), ('dinosaur', 0.027725155)]\n",
      "[('dog', 0.70970178), ('cat', 0.15696621), ('dinosaur', 0.017226743), ('ball', 0.016994534)]\n",
      "Predicting cat.jpg\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "[('cat', 0.99491477), ('dog', 0.0043442352), ('face', 7.9988407e-05), ('fish', 7.9657184e-05)]\n",
      "[('cat', 0.99981731), ('box', 0.00015251513), ('star', 1.3727606e-05), ('keyboard', 3.0921935e-06)]\n",
      "Predicting keyboard.jpg\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "[('blackboard', 0.1212211), ('towel', 0.058512099), ('ruler', 0.049326621), ('shower', 0.04377545)]\n",
      "[('ruler', 0.32487085), ('keyboard', 0.087137252), ('computer mouse', 0.082731299), ('puzzle', 0.076210767)]\n",
      "Predicting macbook.jpg\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "[('keyboard', 0.8350507), ('blackboard', 0.025115665), ('piano', 0.022936612), ('computer mouse', 0.019367529)]\n",
      "[('keyboard', 0.92439377), ('guitar', 0.059645247), ('computer mouse', 0.012432756), ('monitor', 0.0013076052)]\n",
      "Predicting mouse.jpg\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "[('soap', 0.39643952), ('computer mouse', 0.14286114), ('bath', 0.10950692), ('bowl', 0.038996626)]\n",
      "[('soap', 0.36001408), ('computer mouse', 0.23909932), ('pan', 0.090426683), ('hat', 0.051117629)]\n",
      "Predicting banana.jpg\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "[('banana', 0.87966025), ('bowl', 0.018582925), ('pear', 0.016367296), ('onion', 0.0078945532)]\n",
      "[('banana', 0.99367648), ('onion', 0.003206518), ('bowl', 0.00075238926), ('pear', 0.00071624812)]\n"
     ]
    }
   ],
   "source": [
    "import balloonml\n",
    "\n",
    "img_path='test_images/'\n",
    "images = ['calculator.jpg',\n",
    "'dog.jpg',\n",
    "'cat.jpg',\n",
    "'keyboard.jpg',\n",
    "'macbook.jpg',\n",
    "'mouse.jpg',\n",
    "'banana.jpg']\n",
    "for image in images:\n",
    "    print('Predicting '+image)\n",
    "    path = img_path+image\n",
    "    print(balloonml.predict_top(path, weights+'.h5',class_dictionary))\n",
    "    print(balloonml.predict_fine(model,class_dictionary, img_path=path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test images for v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting calculator.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('calculator', 0.021121915), ('keyboard', 0.00038548166), ('key', 6.198656e-05), ('hairbrush', 5.6152505e-05)]\n",
    "[('calculator', 0.2597464), ('keyboard', 0.00038322349), ('key', 9.7186203e-06), ('computer mouse', 2.592287e-06)]\n",
    "Predicting dog.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('cat', 0.0047040172), ('dog', 0.0040679271), ('sofa', 0.00085722323), ('dinosaur', 0.00027150256)]\n",
    "[('cat', 0.0052334545), ('dog', 0.0040698401), ('dinosaur', 0.00085430447), ('sofa', 0.00056111149)]\n",
    "Predicting cat.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('cat', 0.29206869), ('dog', 0.0055200113), ('star', 5.3297485e-05), ('dinosaur', 3.7203841e-05)]\n",
    "[('cat', 0.35923439), ('dog', 0.00079333514), ('star', 2.7750855e-05), ('dinosaur', 1.5055543e-05)]\n",
    "Predicting keyboard.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('radiator', 0.0022044962), ('curtain', 0.00061480148), ('ruler', 0.0005549785), ('blackboard', 0.00050748681)]\n",
    "[('ruler', 0.0010941911), ('keyboard', 0.00091195788), ('computer mouse', 0.00040513425), ('guitar', 0.00040082214)]\n",
    "Predicting macbook.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('keyboard', 0.01001082), ('monitor', 0.00050369196), ('blackboard', 0.00025121815), ('computer mouse', 0.00018567205)]\n",
    "[('keyboard', 0.047883309), ('monitor', 0.00047867949), ('computer mouse', 0.00031897088), ('calculator', 0.00015937244)]\n",
    "Predicting mouse.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('soap', 0.0042663808), ('computer mouse', 0.00091027975), ('bowl', 0.00067775662), ('pan', 0.00041909842)]\n",
    "[('soap', 0.0056782723), ('computer mouse', 0.0010733134), ('bowl', 0.00028223582), ('hat', 0.00015233312)]\n",
    "Predicting banana.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('banana', 0.012315591), ('bowl', 0.00019367522), ('sock', 0.00017960947), ('carrot', 5.7873309e-05)]\n",
    "[('banana', 0.043339983), ('bowl', 0.00051319768), ('onion', 0.00012041288), ('soap', 0.00011019995)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
