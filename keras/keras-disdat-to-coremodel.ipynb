{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:TensorFlow version 1.3.0 detected. Last version known to be fully compatible is 1.2.1 .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import operator\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.callbacks\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import coremltools\n",
    "\n",
    "from PIL import Image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = 'disdat-120-v1-softmax-lr0.001-decay0'\n",
    "label_file='labels.txt'\n",
    "data_folder='../data'\n",
    "output='disdatkerasv8'\n",
    "num_classes=120\n",
    "\n",
    "write_labels=True\n",
    "load_only=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape=(7,7,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116228 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "if write_labels:\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    data_generator = datagen.flow_from_directory(\n",
    "        data_folder,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=11,\n",
    "        class_mode='sparse'\n",
    "    )\n",
    "\n",
    "    class_dictionary = data_generator.class_indices\n",
    "\n",
    "    sorted_predictions = sorted(class_dictionary.items(), key=operator.itemgetter(1))\n",
    "    labels=[p[0] for p in sorted_predictions]\n",
    "\n",
    "    with open(label_file, 'w') as f:\n",
    "        for label in labels:\n",
    "            f.write(label+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('configs/'+weights+'.config') as config_file:    \n",
    "    config=json.load(config_file)\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(num_classes, activation='softmax'))\n",
    "decay=1/100 * (config['lr'])/(100000/config['batch_size'])\n",
    "top_model.compile(optimizer=SGD(lr=config['lr'], decay=config['decay']),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "top_model.load_weights(weights+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs= base_model.input, outputs= top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : input_1, <keras.engine.topology.InputLayer object at 0x7f0ca51f0b50>\n",
      "1 : block1_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0ca51f0b90>\n",
      "2 : block1_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c6076bd10>\n",
      "3 : block1_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0ca51f0c10>\n",
      "4 : block1_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c6076bd50>\n",
      "5 : block1_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c65974fd0>\n",
      "6 : block2_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0c659cc910>\n",
      "7 : block2_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c6076bbd0>\n",
      "8 : block2_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0c6598ea50>\n",
      "9 : block2_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c6076be50>\n",
      "10 : block2_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c659a8f10>\n",
      "11 : block3_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0c65950fd0>\n",
      "12 : block3_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c6076bf10>\n",
      "13 : block3_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0c65962750>\n",
      "14 : block3_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c6076bf90>\n",
      "15 : block3_conv3, <keras.layers.convolutional.Conv2D object at 0x7f0c658f7c90>\n",
      "16 : block3_conv3__activation__, <keras.layers.core.Activation object at 0x7f0c64624050>\n",
      "17 : block3_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c65908dd0>\n",
      "18 : block4_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0c658c4910>\n",
      "19 : block4_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c646240d0>\n",
      "20 : block4_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0c658dc850>\n",
      "21 : block4_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c64624150>\n",
      "22 : block4_conv3, <keras.layers.convolutional.Conv2D object at 0x7f0c65882fd0>\n",
      "23 : block4_conv3__activation__, <keras.layers.core.Activation object at 0x7f0c646241d0>\n",
      "24 : block4_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c6586bf90>\n",
      "25 : block5_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0c658a8450>\n",
      "26 : block5_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c64624250>\n",
      "27 : block5_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0c6583d890>\n",
      "28 : block5_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c646242d0>\n",
      "29 : block5_conv3, <keras.layers.convolutional.Conv2D object at 0x7f0c65851fd0>\n",
      "30 : block5_conv3__activation__, <keras.layers.core.Activation object at 0x7f0c64624350>\n",
      "31 : block5_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c657f8b10>\n",
      "32 : sequential_1_flatten_1, <keras.layers.core.Flatten object at 0x7f0c63efd1d0>\n",
      "33 : sequential_1_dense_1, <keras.layers.core.Dense object at 0x7f0c63efd850>\n",
      "34 : sequential_1_dense_1__activation__, <keras.layers.core.Activation object at 0x7f0c64624390>\n",
      "35 : sequential_1_dense_2, <keras.layers.core.Dense object at 0x7f0c63f096d0>\n",
      "36 : sequential_1_dense_2__activation__, <keras.layers.core.Activation object at 0x7f0c646243d0>\n"
     ]
    }
   ],
   "source": [
    "if load_only:\n",
    "    coreml_model = coremltools.models.MLModel(output+'top.mlmodel')\n",
    "else:\n",
    "    coreml_model  = coremltools.converters.keras.convert(model,\n",
    "                                                        image_input_names='image',\n",
    "                                                        class_labels=label_file,\n",
    "                                                        input_names=['image'],  \n",
    "    #                                                     red_bias=-123.682/255,\n",
    "    #                                                     green_bias=-116.78/255,\n",
    "    #                                                     blue_bias=-103.94/255,\n",
    "                                                        image_scale=1/255.0\n",
    "    )\n",
    "    \n",
    "# Set model metadata\n",
    "coreml_model.author = 'Balloon Inc. VOF'\n",
    "coreml_model.short_description = 'Custom object detection for 120 simple classes.'\n",
    "\n",
    "\n",
    "coreml_model.save(output+'top.mlmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "refined_model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "refined_model.load_weights(weights+'-refined.h5')\n",
    "refined_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=2e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : input_2, <keras.engine.topology.InputLayer object at 0x7f0c64624810>\n",
      "1 : block1_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0c607eadd0>\n",
      "2 : block1_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c6076bc50>\n",
      "3 : block1_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0c64624850>\n",
      "4 : block1_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c6076bdd0>\n",
      "5 : block1_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c646248d0>\n",
      "6 : block2_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0c645d7ed0>\n",
      "7 : block2_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c6076bd90>\n",
      "8 : block2_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0c6456dc50>\n",
      "9 : block2_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c6076bd50>\n",
      "10 : block2_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c6457ee50>\n",
      "11 : block3_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0c64593850>\n",
      "12 : block3_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c6076bf90>\n",
      "13 : block3_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0c645a4bd0>\n",
      "14 : block3_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c6076bd10>\n",
      "15 : block3_conv3, <keras.layers.convolutional.Conv2D object at 0x7f0c64539f50>\n",
      "16 : block3_conv3__activation__, <keras.layers.core.Activation object at 0x7f0c63ef4850>\n",
      "17 : block3_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c6455e8d0>\n",
      "18 : block4_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0c64508a50>\n",
      "19 : block4_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c63ef4390>\n",
      "20 : block4_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0c64517590>\n",
      "21 : block4_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c63ef4e10>\n",
      "22 : block4_conv3, <keras.layers.convolutional.Conv2D object at 0x7f0c644ac910>\n",
      "23 : block4_conv3__activation__, <keras.layers.core.Activation object at 0x7f0c64624710>\n",
      "24 : block4_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c644c0550>\n",
      "25 : block5_conv1, <keras.layers.convolutional.Conv2D object at 0x7f0c6447bed0>\n",
      "26 : block5_conv1__activation__, <keras.layers.core.Activation object at 0x7f0c64624410>\n",
      "27 : block5_conv2, <keras.layers.convolutional.Conv2D object at 0x7f0c64491c50>\n",
      "28 : block5_conv2__activation__, <keras.layers.core.Activation object at 0x7f0c646244d0>\n",
      "29 : block5_conv3, <keras.layers.convolutional.Conv2D object at 0x7f0c644a4e50>\n",
      "30 : block5_conv3__activation__, <keras.layers.core.Activation object at 0x7f0c64624290>\n",
      "31 : block5_pool, <keras.layers.pooling.MaxPooling2D object at 0x7f0c6443ac10>\n",
      "32 : sequential_2_flatten_2, <keras.layers.core.Flatten object at 0x7f0c6441a710>\n",
      "33 : sequential_2_dense_3, <keras.layers.core.Dense object at 0x7f0c6441a750>\n",
      "34 : sequential_2_dense_3__activation__, <keras.layers.core.Activation object at 0x7f0c64624110>\n",
      "35 : sequential_2_dense_4, <keras.layers.core.Dense object at 0x7f0c643d2c50>\n",
      "36 : sequential_2_dense_4__activation__, <keras.layers.core.Activation object at 0x7f0c64624310>\n"
     ]
    }
   ],
   "source": [
    "if load_only:\n",
    "    coreml_model_refined = coremltools.models.MLModel(output+'.mlmodel')\n",
    "else:\n",
    "    coreml_model_refined  = coremltools.converters.keras.convert(refined_model,\n",
    "                                                        image_input_names='image',\n",
    "                                                        class_labels=label_file,\n",
    "                                                        input_names=['image'],  \n",
    "    #                                                     red_bias=-123.68/255,\n",
    "    #                                                     green_bias=-116.78/255,\n",
    "    #                                                     blue_bias=-103.94/255,\n",
    "                                                        image_scale=1/255.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set model metadata\n",
    "coreml_model_refined.author = 'Balloon Inc. VOF'\n",
    "coreml_model_refined.short_description = 'Custom object detection for 120 simple classes.'\n",
    "\n",
    "coreml_model_refined.save(output+'-refined.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import balloonml\n",
    "\n",
    "img_path='test_images/'\n",
    "images = ['calculator.jpg',\n",
    "'dog.jpg',\n",
    "'cat.jpg',\n",
    "'keyboard.jpg',\n",
    "'macbook.jpg',\n",
    "'mouse.jpg',\n",
    "'banana.jpg']\n",
    "for image in images:\n",
    "    print('Predicting '+image)\n",
    "    path = img_path+image\n",
    "    # keras prediction\n",
    "    print(balloonml.predict_top(path, weights+'.h5',class_dictionary))\n",
    "    print(balloonml.predict_fine(model,class_dictionary, img_path=path))\n",
    "    \n",
    "    # CoreML prediction\n",
    "    im = Image.open(path)  \n",
    "    coreml_model.predict({'data': im})  \n",
    "    coreml_model_refined.predict({'data': im})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test images for v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting calculator.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('calculator', 0.021121915), ('keyboard', 0.00038548166), ('key', 6.198656e-05), ('hairbrush', 5.6152505e-05)]\n",
    "[('calculator', 0.2597464), ('keyboard', 0.00038322349), ('key', 9.7186203e-06), ('computer mouse', 2.592287e-06)]\n",
    "Predicting dog.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('cat', 0.0047040172), ('dog', 0.0040679271), ('sofa', 0.00085722323), ('dinosaur', 0.00027150256)]\n",
    "[('cat', 0.0052334545), ('dog', 0.0040698401), ('dinosaur', 0.00085430447), ('sofa', 0.00056111149)]\n",
    "Predicting cat.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('cat', 0.29206869), ('dog', 0.0055200113), ('star', 5.3297485e-05), ('dinosaur', 3.7203841e-05)]\n",
    "[('cat', 0.35923439), ('dog', 0.00079333514), ('star', 2.7750855e-05), ('dinosaur', 1.5055543e-05)]\n",
    "Predicting keyboard.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('radiator', 0.0022044962), ('curtain', 0.00061480148), ('ruler', 0.0005549785), ('blackboard', 0.00050748681)]\n",
    "[('ruler', 0.0010941911), ('keyboard', 0.00091195788), ('computer mouse', 0.00040513425), ('guitar', 0.00040082214)]\n",
    "Predicting macbook.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('keyboard', 0.01001082), ('monitor', 0.00050369196), ('blackboard', 0.00025121815), ('computer mouse', 0.00018567205)]\n",
    "[('keyboard', 0.047883309), ('monitor', 0.00047867949), ('computer mouse', 0.00031897088), ('calculator', 0.00015937244)]\n",
    "Predicting mouse.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('soap', 0.0042663808), ('computer mouse', 0.00091027975), ('bowl', 0.00067775662), ('pan', 0.00041909842)]\n",
    "[('soap', 0.0056782723), ('computer mouse', 0.0010733134), ('bowl', 0.00028223582), ('hat', 0.00015233312)]\n",
    "Predicting banana.jpg\n",
    "1/1 [==============================] - 0s\n",
    "1/1 [==============================] - 0s\n",
    "[('banana', 0.012315591), ('bowl', 0.00019367522), ('sock', 0.00017960947), ('carrot', 5.7873309e-05)]\n",
    "[('banana', 0.043339983), ('bowl', 0.00051319768), ('onion', 0.00012041288), ('soap', 0.00011019995)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
